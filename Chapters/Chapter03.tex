% Chapter 3

\chapter{Recursive Partitioning and Multilevel Data} % Chapter title

\label{ch:previous} % For referencing the chapter elsewhere, use \autoref{ch:previous} 

	There is a small, but growing body of research that examines the application of decision trees to complex data structures. \citeA{segal1992tree} provides an initial foray into this topic by attempting to extend the logic of decision trees and covariate splits to longitudinal data. In this method, split functions for trajectories can be directed toward either the mean vector (e.g., an intercept and a slope term) or the covariance matrix. One large difficulty in this procedure is the handling of time-varying covariates. Because changes in the covariance structure is considered in the splitting function, potential splits can only be done at the participant level and not the observation level. Thus, time-varying covariates can only be included if they are aggregated to the participant-level of analysis as a low-order polynomial term. That is, using an intercept and slope to approximate the trajectory of the time-varying covariate, and then using these new variables as potential variables to split on \cite{segal1992tree}. This is an important methodological consideration that will manifest itself throughout this discussion of previous research.


	This method can also be utilized to identify representative curves for the purpose of exploratory data analysis \cite{segal1994representative}. While just sampling curves can be problematic in a sense that potential non-representative trajectories are selected (i.e., outliers), this method can detect homogeneous sub-populations of trajectories in regard to both the outcome and a set of covariates. A similar approach can be made from an unsupervised perspective, where representative groups are selected based on trajectories only and not the covariates themselves \cite{martin2014growth, tucker1966}.


	Other methods focused on applying recursive partitioning to longitudinal data employed improved algorithms that do not exhibit the selection bias commonly found in some recursive partitioning algorithms, such as CART. For example, \citeA{eo2013tree} proposed a recursive partitioning algorithm based on GUIDE \cite{loh2002regression}, which utilizes residual analysis to avoid selection bias in its splitting procedure and a cost-complexity stopping rule instead of cross-validation to save on computation time. Similar to \citeA{segal1992tree}, this method can only identify trends, not predict future responses. As such, it is limited in only being able to split on variables at the cluster level, not the observation level. A similar method proposed by \citeA{loh2013regression} also utilized GUIDE for this purpose.


	Note that these aforementioned methods focused on exploratory data analyses in a multilevel framework that is longitudinal in nature (i.e., repeated observations nested within participants). Emphasis is also being placed on predictive accuracy in a multilevel framework that is typically cross-sectional in nature. For example, \citeA{karpievitch2009introspective} examined the classification performance of random forests in mass spectrometry-based studies, which often produces cluster-correlated data. Via simulation, they found that traditional random forest algorithms yielded good classification performance despite the presence of cluster-correlated data, except that the OOB error rate typically underestimated the actual error rate calculated on a separate test data set. By extending the original random forest algorithm to instead re-sample at the cluster level rather than the observation level led to near identical median classification rates and variable selection accuracy when compared to the original random forest algorithm, but without the bias found in the OOB error rate. The conceptual background for why this occurs will be addressed in \autoref{ch:problem}.  


	Additional examples that also focus on the performance of resampling methods for random forests in cluster-correlated data are typically found in medical research. In one example, \citeA{adler2011ensemble} compared different resampling methods for random forests in the presence of paired organ data, and found that the decrease in performance at high correlations between organs can be reduced if a paired bootstrap is performed. It has also been reported that sampling one observation rather than resampling all observations within an individual can lead to better performance when classifying future observations of patients in a longitudinal framework \cite{adler2011classification}.


	More recently, researchers have also begun to focus on estimating and using a random effects structure in tandem with decision trees to improve predictions. For example, both \citeA{sela2012re} and \citeA{hajjem2011mixed} independently proposed the same method to incorporate a given random effects structure in a recursive partitioning algorithm, called RE-EM trees and mixed-effects regression trees, respectively. Both approaches operate on the same algorithm, namely one that uses a variant of the EM algorithm to estimate a set of random effects (most commonly just random intercepts) to encompass an entire tree. While any underlying decision tree algorithm can be used, both authors adopted an approach using CART. Because these random effects can be used to alter predictions for each individual observation in the sample after filtering through the tree structure, both methods show increased in-sample predictive accuracy compared to both a traditional multilevel model with main effects or a decision tree without this random effect structure. However, because random effects have a mean of zero, the predictive performance for observations nested within unobserved clusters between a decision tree with and without a random effects structure was similar \cite{hajjem2011mixed, sela2012re}.


	\citeA{hajjem2014mixed} have also extended these trees to create an ensemble method referred to as mixed-effects random forests. This method first removes the estimated random effects in the model before performing bootstrapping to avoid employing a cluster bootstrap. It also finds substantial improvements over single decision trees both with or without random effects in terms of prediction accuracy. Given the bias inherent in the splitting procedure of the CART algorithm, these methods have also been built around conditional inference trees, resulting in unbiased variable selection with the added benefit of having a random effects structure to improve predictive accuracy. Despite the unbiased variable selection, the conditional inference tree with random effects still yields similar predictive accuracy when compared to a CART tree with random effects \cite{fu2014unbiased}; a similar scenario to how these methods perform without a random effects structure \cite{strobl2007bias}.

	Additional research has investigated decision trees in multilevel contexts from a model-based perspective. That is, rather than focus on prediction with a basic tree approach where each node just represents a different mean value (in the case of a regression tree) or class (in the case of a classification tree), an alternative is to have a statistical model in each node \cite{zeileis2008model}. For example, a linear regression tree searches the covariate space for potential splits that maximizes the difference in model parameters in an already specified model. Thus, it becomes possible to unite confirmatory and exploratory research and examine potential model misspecification in more detail \cite{kopf2010potential}. Recently, model-based decision trees have been extended to a structural equation modeling (SEM) framework, called SEM trees \cite{brandmaier2013structural}. In this method, decision trees are combined with the flexibility of SEM, where SEMs are split based on a set of covariates, effectively creating a series of more homogenous multi-group models. This model-based recursive partitioning method also avoids selection bias with a two stage procedure found in \citeA{loh1997split}, which first selects the best cut point for each variable and then selects the best split among these candidates. Like many trees methods, creating an ensemble (i.e., a SEM forest) helps to reduce bias and increase the stability of estimated relationships among variables \cite{prindle2014}. Because this framework also estimates variance components, it is limited to only splitting on cluster level variables in the presence of clustered data. 

